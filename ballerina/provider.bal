// Copyright (c) 2025 WSO2 LLC (http://www.wso2.com).
//
// WSO2 LLC. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

import ballerina/ai;
import ballerina/http;
import ballerina/time;

const DEFAULT_DEEPSEEK_SERVICE_URL = "https://api.deepseek.com";
const DEFAULT_MAX_TOKEN_COUNT = 512;
const DEFAULT_TEMPERATURE = 0.7d;

# Deepseek is a client class that provides an interface for interacting with Deepseek Large Language Models.
public isolated client class Provider {
    *ai:ModelProvider;
    private final http:Client llmClient;
    private final int maxTokens;
    private final DEEPSEEK_MODEL_NAMES modelType;

    # Initializes the Deepseek model client with the provided configurations.
    #
    # + apiKey - The Deepseek API Key
    # + model - The Deepseek model name
    # + serviceUrl - The base URL of Deepseek API endpoint
    # + maxTokens - The upper limit for the number of tokens in the response generated by the model
    # + temperateure - The temperature for controlling randomness in the model's output
    # + connectionConfig - Additional HTTP client configurations
    # + return - `nil` on successful initialization; otherwise, returns an `ai:Error`
    public isolated function init(@display {label: "API Key"} string apiKey,
            @display {label: "Model Type"} DEEPSEEK_MODEL_NAMES modelType = DEEPSEEK_CHAT,
            @display {label: "Service URL"} string serviceUrl = DEFAULT_DEEPSEEK_SERVICE_URL,
            @display {label: "Maximum Token"} int maxTokens = DEFAULT_MAX_TOKEN_COUNT,
            @display {label: "Temperature"} decimal temperateure = DEFAULT_TEMPERATURE,
            @display {label: "Connection Configuration"} *ConnectionConfig connectionConfig
    ) returns ai:Error? {

        http:ClientConfiguration deepseekConfig = {
            auth: {token: apiKey},
            httpVersion: connectionConfig.httpVersion,
            http1Settings: connectionConfig.http1Settings ?: {},
            http2Settings: connectionConfig?.http2Settings ?: {},
            timeout: connectionConfig.timeout,
            forwarded: connectionConfig.forwarded,
            poolConfig: connectionConfig?.poolConfig,
            cache: connectionConfig?.cache ?: {},
            compression: connectionConfig.compression,
            circuitBreaker: connectionConfig?.circuitBreaker,
            retryConfig: connectionConfig?.retryConfig,
            responseLimits: connectionConfig?.responseLimits ?: {},
            secureSocket: connectionConfig?.secureSocket,
            proxy: connectionConfig?.proxy,
            validation: connectionConfig.validation
        };

        http:Client|error httpClient = new http:Client(serviceUrl, deepseekConfig);

        if httpClient is error {
            return error ai:Error("Failed to initialize Deepseek client", httpClient);
        }

        self.maxTokens = maxTokens;
        self.modelType = modelType;
        self.llmClient = httpClient;
    }

    # Generates a chat completion message from a Deepseek model
    #
    # + messages - List of chat messages 
    # + tools - Tool definitions to be used for the tool call
    # + stop - Stop sequence to stop the completion
    # + return - Function to be called, chat response or an error in-case of failures
    isolated remote function chat(ai:ChatMessage[] messages, ai:ChatCompletionFunctions[] tools, string? stop = ())
    returns ai:ChatAssistantMessage|ai:LlmError {
        DeepSeekChatRequestMessages[] deepseekPayloadMessages = check self.prepareDeepseekRequestMessages(messages);

        DeepSeekChatCompletionRequest request = {
            messages: deepseekPayloadMessages,
            model: self.modelType,
            max_tokens: self.maxTokens,
            stop: stop
        };

        if tools.length() > 0 {
            DeepseekFunction[] deepseekFunctions = [];
            foreach ai:ChatCompletionFunctions toolFunction in tools {
                DeepseekFunction deepseekFunction = {
                    name: toolFunction.name,
                    description: toolFunction.description,
                    parameters: toolFunction.parameters ?: {}
                };
                deepseekFunctions.push(deepseekFunction);
            }
            DeepseekTool[] deepseekTools = deepseekFunctions.'map(self.transFormFuncToTool);
            request.tools = deepseekTools;
        }

        DeepSeekChatCompletionResponse|error response = self.llmClient->/chat/completions.post(request);
        if response is error {
            return error ai:LlmConnectionError("Error while connecting to the model", response);
        }
        return self.getAssistantMessages(response);
    }

    private isolated function transFormFuncToTool(DeepseekFunction deepseekFunction) returns DeepseekTool {
        DeepseekTool deepseekTool = {'function: deepseekFunction};
        return deepseekTool;
    }

    # Generates a random tool ID.
    #
    # + return - A random tool ID string
    private isolated function generateNewToolId() returns string {
        decimal timestampForToolCallId = time:monotonicNow();
        string strTimestamp = timestampForToolCallId.toString();
        string toolCallId = re `\.`.replace(strTimestamp, "");
        string newToolCallId = toolCallId.substring(toolCallId.length() - 9);
        return newToolCallId;
    }

    # Retrieves the ID of the last tool call from a deepseek chat assistant message.
    #
    # + deepseekPayloadMessages - Array of DeepSeek chat messages
    # + return - ID of the first tool call from the most recent assistant message, or empty `ai:Error`
    private isolated function getAssistantToolCallId(DeepSeekChatRequestMessages[] deepseekPayloadMessages)
    returns string|ai:LlmError {
        // Iterate from the end of the array to find the last assistant message
        int index = deepseekPayloadMessages.length() - 1;
        while (index >= 0) {
            DeepSeekChatRequestMessages message = deepseekPayloadMessages[index];
            if message is DeepseekChatAssistantMessage && message.tool_calls is DeepseekChatResponseToolCall[] {
                DeepseekChatResponseToolCall[]? tools = message.tool_calls;
                if tools is () {
                    return error ai:LlmError("No tool calls found in the assistant message");
                }
                if tools.length() > 0 {
                    return tools[0].id;
                }
            }
            index = index - 1;
        }
        return "";
    }

    # Maps an array of `ai:ChatMessage` records to corresponding Mistral message records.
    #
    # + messages - Array of chat messages to be converted
    # + return - An `ai:LlmError` or an array of Mistral message records
    private isolated function prepareDeepseekRequestMessages(ai:ChatMessage[] messages)
        returns DeepSeekChatRequestMessages[]|ai:LlmError {
        DeepSeekChatRequestMessages[] deepseekMessages = [];
        foreach ai:ChatMessage message in messages {
            if message is ai:ChatUserMessage {
                DeepseekChatUserMessage deepseekUserMessage = {
                    role: ai:USER,
                    content: message.content
                };
                deepseekMessages.push(deepseekUserMessage);
            } else if message is ai:ChatSystemMessage {
                DeepseekChatSystemMessage deepseekSysteMessage = {
                    role: ai:SYSTEM,
                    content: message.content
                };
                deepseekMessages.push(deepseekSysteMessage);
            } else if message is ai:ChatAssistantMessage {
                ai:FunctionCall[]? toolCalls = message.toolCalls;
                DeepseekChatAssistantMessage deepseekAssistantMessage = {role: ai:ASSISTANT, content: message.content};
                if toolCalls is ai:FunctionCall[] {
                    DeepseekChatResponseToolCall[] toolCall = [];
                    foreach ai:FunctionCall 'function in toolCalls {
                        DeepseekChatResponseFunction functionCall = {
                            name: 'function.name,
                            arguments: 'function.arguments.toJsonString()
                        };
                        DeepseekChatResponseToolCall tool = {
                            'function: functionCall,
                            id: 'function.id ?: self.generateNewToolId(),
                            'type: ai:FUNCTION
                        };
                        toolCall.push(tool);
                    }
                    deepseekAssistantMessage.tool_calls = toolCall;
                }
                deepseekMessages.push(deepseekAssistantMessage);
            } else if message is ai:ChatFunctionMessage {
                DeepseekChatToolMessage deepseekToolMessage = {
                    role: TOOL_ROLE,
                    content: message?.content is string ? {result: message.content}.toJsonString() : "",
                    tool_call_id: message.id ?: check self.getAssistantToolCallId(deepseekMessages)
                };
                deepseekMessages.push(deepseekToolMessage);
            }
        }
        return deepseekMessages;
    }

    # Extracts assistant messages from a Deepseek chat completion response.
    #
    # + response - The response from LLM
    # + return - Chat response or an error in-case of failures
    private isolated function getAssistantMessages(DeepSeekChatCompletionResponse response)
    returns ai:ChatAssistantMessage|ai:LlmError {
        DeepseekChatResponseChoice[]? choices = response.choices;
        if choices is () || choices.length() == 0 {
            return error ai:LlmInvalidResponseError("Empty response from the model when using function call API");
        }
        DeepseekChatResponseMessage message = choices[0].message;
        string? content = message?.content;
        DeepseekChatResponseToolCall[]? toolCalls = message?.tool_calls;
        if toolCalls is () {
            return {role: ai:ASSISTANT, content: content};
        }

        ai:FunctionCall[] functionCalls = [];
        foreach DeepseekChatResponseToolCall toolCall in toolCalls {
            functionCalls.push(check self.mapToFunctionCall(toolCall));
        }
        return {role: ai:ASSISTANT, toolCalls: functionCalls, content: content};
    }

    private isolated function mapToFunctionCall(DeepseekChatResponseToolCall toolCall)
    returns ai:FunctionCall|ai:LlmError {
        do {
            json jsonArgs = check toolCall.'function.arguments.fromJsonString();
            map<json>? arguments = check jsonArgs.cloneWithType();
            return {name: toolCall.'function.name, arguments, id: toolCall.id};
        } on fail error e {
            return error ai:LlmError("Invalid or malformed arguments received in function call response.", e);
        }
    }
}
